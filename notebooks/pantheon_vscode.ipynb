{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quantifying Uncertainty with Markov Chain Monte Carlo Sampling\n",
    "\n",
    "__Benjamin Pope__\n",
    "\n",
    "_University of Queensland_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is going to be an interactive lecture using Jupyter! \n",
    "# So let's import things!\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from astropy.table import Table\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import emcee\n",
    "import chainconsumer\n",
    "from chainconsumer import ChainConsumer, Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Hubble's Original Data\n",
    "\n",
    "Here are some distances from Edwin Hubble's [1929 paper](https://www.pnas.org/doi/pdf/10.1073/pnas.15.3.168) discovering the expansion of the universe. \n",
    "\n",
    "<img src=\"hubble.png\" alt=\"Hubble's Data\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this lecture, we're going to learn how to fit a model to data like these - and to quantify their uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and plot Hubble original data\n",
    "\n",
    "data = Table.read('../data/hubble.txt', format='ascii')\n",
    "d, v = data['r'], data['v']\n",
    "d_err = 0.1\n",
    "plt.errorbar(d,v,xerr=d_err,fmt='o',color='k')\n",
    "plt.xlabel('Distance (Mpc)')\n",
    "plt.ylabel('RV (km/s)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In physics we gather data through experiment and observation; and do theoretical work to calculate how the outcomes of experiments depend on parameters of their models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The job of data analysts in physics is to connect the two, solving the inverse problem to infer parameters of a theory from empirical data - and it is usually just as important to quantify the uncertainty on these parameters as to just find a best fitting model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The most common way to do this is with a Markov Chain Monte Carlo algorithm, which gives us a set of samples drawn from this probability distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "In this lecture I will discuss the theory and the surprising history behind the now-ubiquitous MCMC, and illustrate this with an interactive session fitting Hubble's original discovery of the expanding universe with a simple model in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What do we mean by Monte Carlo methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Stan Ulam\n",
    "\n",
    "<img src=\"ulam.png\" alt=\"MCMC Paper\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Metropolis-Hastings Algorithm\n",
    "\n",
    "<img src=\"mr2t2.png\" alt=\"MCMC Paper\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Arianna Rosenbluth\n",
    "\n",
    "<img src=\"rosenbluth.jpeg\" alt=\"Arianna Rosenbluth\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<img src=\"rosenbluth_headline.png\" alt=\"Arianna Rosenbluth\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Metropolis-Hastings rules\n",
    "\n",
    "- Ergodic: A Markov chain has a limiting distribution if it is \n",
    "    - Irreducible: All states can be reached from other states\n",
    "    - Aperiodic: Does not have a period of repeat\n",
    "    - Positive recurrent: Expected to return (close to) any state in a finite number of steps\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Will converge to the true distribution if ergodic and has\n",
    "\n",
    "_Detailed balance_: Probability of transition forwards and backwards are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So let's do MCMC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first define priors\n",
    "\n",
    "def lnprior(H0):\n",
    "    # prior probability\n",
    "    if 0 < H0 < 1000:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def model(H0, v):\n",
    "    # forward model\n",
    "    return v/H0\n",
    "\n",
    "def lnlike(H0, v, d, d_err):\n",
    "    # log-likelihood function\n",
    "    return -0.5 * np.sum((d-model(H0,v))**2 / d_err**2)\n",
    "\n",
    "def lnprob(theta, v, d, d_err):\n",
    "    # log probability = log prior + log likelihood\n",
    "    H0 = theta\n",
    "    lp = lnprior(H0)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(H0, v, d, d_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample with emcee\n",
    "ndim, nwalkers = 1, 100\n",
    "pos = 500 + np.random.randn(nwalkers, ndim)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(v, d, d_err))\n",
    "burnin = sampler.run_mcmc(pos, 500, progress=True)\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(burnin, 1000, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot history - should look like noise\n",
    "\n",
    "samples = sampler.get_chain(flat=True)\n",
    "chain = Chain.from_emcee(sampler, ['H0'], \"an emcee chain\", discard=200, thin=2, color=\"indigo\")\n",
    "consumer = ChainConsumer().add_chain(chain)\n",
    "\n",
    "fig = consumer.plotter.plot_walks(plot_weights=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot posterior histogram - so far only one variable\n",
    "fig = consumer.plotter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot posterior predictive model\n",
    "\n",
    "inds = np.random.choice(np.arange(samples.shape[0]), 50)\n",
    "ds = np.linspace(0, 2.3, 100)\n",
    "\n",
    "for ind in inds:\n",
    "    H0 = samples[ind,0]\n",
    "\n",
    "    v_model = H0*ds\n",
    "    plt.plot(ds, v_model, 'g-', alpha=0.05)\n",
    "\n",
    "plt.errorbar(d, v,  xerr=d_err, fmt='.', capsize=2,color='k')\n",
    "plt.xlim(0, ds.max())\n",
    "\n",
    "plt.xlabel('Distance (Mpc)')\n",
    "plt.ylabel('Velocity (km/s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Now let's do the Pantheon dataset!\n",
    "\n",
    "This is a large curated catalogue of modern supernova cosmology data. \n",
    "\n",
    "We'll only look in the local universe, where we don't have to solve the full FRW equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load Pantheon Plus dataset\n",
    "\n",
    "ddir = '../data/'\n",
    "fname = 'Pantheon.dat'\n",
    "\n",
    "data = Table.read(ddir+fname, format='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in the Pantheon Plus data\n",
    "z = data['zCMB']\n",
    "z_err = data['zCMBERR']\n",
    "mb = data['MU_SH0ES'] # distance modulus\n",
    "mb_err = data['MU_SH0ES_ERR_DIAG']\n",
    "\n",
    "cut = (z > 0.02) & (z < 0.06) & (z_err < 0.005)\n",
    "z = z[cut]\n",
    "z_err = z_err[cut]\n",
    "mb = mb[cut]\n",
    "mb_err = mb_err[cut]\n",
    "\n",
    "# distance\n",
    "d = 10**((mb-25)/5) # Mpc\n",
    "d_err = d * np.log(10) * mb_err / 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot Pantheon Plus data\n",
    "\n",
    "plt.errorbar(z, d, yerr=d_err, xerr=z_err, fmt='.', capsize=2,color='k')\n",
    "zs = np.linspace(0, 0.0602, 100)\n",
    "# Hubble's law\n",
    "# H0 = 75 km/s/Mpc\n",
    "# z to velocity\n",
    "c = 299792.458 # km/s\n",
    "# Relativistic doppler shift\n",
    "vs = c * (zs / (1 + zs))\n",
    "plt.xlim(0.018,0.0602)\n",
    "plt.xlabel('Redshift')\n",
    "plt.ylabel('Distance (Mpc)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lnprior(H0):\n",
    "    if 0 < H0 < 200:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def model(H0,intercept,z):\n",
    "    # now we are including an intercept, ie y = mx + b\n",
    "    v = c * (z / (1 + z))\n",
    "    return v/H0 + intercept\n",
    "\n",
    "def lnlike(H0, intercept, z, d, d_err):\n",
    "    return -0.5 * np.sum((d-model(H0,intercept,z))**2 / d_err**2)\n",
    "\n",
    "def lnprob(theta, z, d, d_err):\n",
    "    H0, intercept = theta\n",
    "    lp = lnprior(H0)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(H0,intercept, z, d, d_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample with emcee\n",
    "ndim, nwalkers = 2, 100 # notice we have 2 params \n",
    "pos = np.array([68,0]) + 1e-4 * np.random.randn(100, 2)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(z, d, d_err))\n",
    "burnin = sampler.run_mcmc(pos, 500, progress=True)\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(burnin, 1000, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chainconsumer\n",
    "\n",
    "samples = sampler.get_chain(flat=True)\n",
    "chain = Chain.from_emcee(sampler, ['H0','intercept'], \"an emcee chain\", discard=200, thin=2, color=\"indigo\")\n",
    "consumer = ChainConsumer().add_chain(chain)\n",
    "\n",
    "fig = consumer.plotter.plot_walks(plot_weights=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is a corner plot, illustrating covariance\n",
    "fig = consumer.plotter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot posterior predictive model\n",
    "\n",
    "# choose values\n",
    "\n",
    "inds = np.random.choice(np.arange(samples.shape[0]), 50)\n",
    "zs = np.linspace(0, 0.061, 100)\n",
    "\n",
    "for ind in inds:\n",
    "    H0 = samples[ind,0]\n",
    "    intercept = samples[ind,1]\n",
    "\n",
    "    d_model = model(H0, intercept, zs)\n",
    "    plt.plot(zs, d_model, 'g-', alpha=0.05)\n",
    "\n",
    "plt.errorbar(z, d, yerr=d_err, xerr=z_err, fmt='.', capsize=2,color='k')\n",
    "plt.xlim(0.0,0.0602)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chi2 = -2 * lnlike(69,0, z, d, d_err)\n",
    "chi2_red = chi2 / (len(z) - 1)\n",
    "chi2_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lnprior(H0):\n",
    "    if 0 < H0 < 200:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def model(H0,quadratic,intercept,z):\n",
    "    v = c * (z / (1 + z))\n",
    "    return v/H0 + intercept + v**2 * quadratic\n",
    "\n",
    "def lnlike(H0, quadratic, intercept, z, d, d_err):\n",
    "    return -0.5 * np.sum((d-model(H0,quadratic,intercept,z))**2 / d_err**2)\n",
    "\n",
    "def lnprob(theta, z, d, d_err):\n",
    "    H0, quadratic, intercept = theta\n",
    "    lp = lnprior(H0)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(H0,quadratic,intercept, z, d, d_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample with emcee\n",
    "ndim, nwalkers = 3, 100\n",
    "pos = np.array([68,0,0]) + 1e-4 * np.random.randn(100, 3)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(z, d, d_err))\n",
    "burnin = sampler.run_mcmc(pos, 500, progress=True)\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(burnin, 1000, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chainconsumer\n",
    "\n",
    "samples = sampler.get_chain(flat=True)\n",
    "chain = Chain.from_emcee(sampler, ['H0','quadratic','intercept'], \"an emcee chain\", discard=200, thin=2, color=\"indigo\")\n",
    "consumer = ChainConsumer().add_chain(chain)\n",
    "fig = consumer.plotter.plot_walks(plot_weights=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we now have a higher dimensional corner plot\n",
    "fig = consumer.plotter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot posterior predictive model\n",
    "\n",
    "inds = np.random.choice(np.arange(samples.shape[0]), 50)\n",
    "zs = np.linspace(0, 0.061, 100)\n",
    "\n",
    "for ind in inds:\n",
    "    H0 = samples[ind,0]\n",
    "    quadratic = samples[ind,1]\n",
    "    intercept = samples[ind,2]\n",
    "\n",
    "    d_model = model(H0, quadratic, intercept, zs)\n",
    "    plt.plot(zs, d_model, 'g-', alpha=0.05)\n",
    "\n",
    "plt.errorbar(z, d, yerr=d_err, xerr=z_err, fmt='.', capsize=2,color='k')\n",
    "plt.xlim(0.0,0.0602)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
